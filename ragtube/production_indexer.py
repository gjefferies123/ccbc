#!/usr/bin/env python3
"""Production-ready Pinecone indexer using modern API."""

import os
import json
import numpy as np
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment
load_dotenv()

class ProductionIndexer:
    """Production Pinecone indexer with modern API."""
    
    def __init__(self):
        self.api_key = os.getenv("PINECONE_API_KEY")
        self.index_name = os.getenv("PINECONE_INDEX", "christ-chapel-sermons")
        self.cohere_api_key = os.getenv("COHERE_API_KEY")
        
        if not self.api_key:
            raise ValueError("PINECONE_API_KEY not found")
        if not self.cohere_api_key:
            raise ValueError("COHERE_API_KEY not found")
        
        self.pc = None
        self.index = None
        self.cohere_client = None
        
        self._init_clients()
    
    def _init_clients(self):
        """Initialize clients using modern API."""
        
        # Initialize Cohere
        try:
            import cohere
            self.cohere_client = cohere.Client(self.cohere_api_key)
            logger.info("‚úÖ Cohere client ready")
        except Exception as e:
            raise RuntimeError(f"Cohere init failed: {e}")
        
        # Initialize Pinecone with modern API
        try:
            from pinecone import Pinecone, ServerlessSpec
            
            self.pc = Pinecone(api_key=self.api_key)
            logger.info("‚úÖ Pinecone client ready")
            
            # Check if index exists
            existing_indexes = [idx.name for idx in self.pc.list_indexes()]
            
            if self.index_name not in existing_indexes:
                logger.info(f"üîß Creating index: {self.index_name}")
                
                self.pc.create_index(
                    name=self.index_name,
                    dimension=1024,  # Cohere embed-english-v3.0
                    metric="cosine",
                    spec=ServerlessSpec(
                        cloud="aws",
                        region="us-west-2"
                    )
                )
                logger.info(f"‚úÖ Index {self.index_name} created")
            
            # Connect to index
            self.index = self.pc.Index(self.index_name)
            logger.info(f"‚úÖ Connected to index: {self.index_name}")
            
        except ImportError:
            logger.error("‚ùå Modern Pinecone package not available")
            logger.info("üí° Using mock mode for development")
            from robust_indexer import MockPineconeIndex
            self.index = MockPineconeIndex()
            
        except Exception as e:
            logger.error(f"‚ùå Pinecone connection failed: {e}")
            logger.info("üí° Using mock mode")
            from robust_indexer import MockPineconeIndex
            self.index = MockPineconeIndex()
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """Create Cohere embeddings in batches."""
        batch_size = 96  # Cohere limit
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            response = self.cohere_client.embed(
                texts=batch,
                model="embed-english-v3.0",
                input_type="search_document"
            )
            
            batch_embeddings = np.array(response.embeddings)
            all_embeddings.append(batch_embeddings)
            
            logger.info(f"‚úÖ Embeddings batch {i//batch_size + 1}: {batch_embeddings.shape}")
        
        embeddings = np.vstack(all_embeddings)
        logger.info(f"üìä Total embeddings: {embeddings.shape}")
        return embeddings
    
    def clear_index(self):
        """Clear the index."""
        try:
            if hasattr(self.index, 'delete'):
                self.index.delete(delete_all=True)
                logger.info("‚úÖ Index cleared")
            else:
                logger.info("‚ÑπÔ∏è Mock index - no clearing needed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Clear failed: {e}")
    
    def prepare_vectors(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Prepare vectors with rich metadata."""
        logger.info(f"üîß Preparing {len(chunks)} vectors...")
        
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.create_embeddings(texts)
        
        vectors = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            
            # Create rich metadata for better search
            vector = {
                'id': chunk['id'],
                'values': embedding.tolist(),
                'metadata': {
                    # Core identifiers
                    'video_id': chunk['video_id'],
                    'chunk_index': i,
                    
                    # Temporal data
                    'start_sec': float(chunk['start_sec']),
                    'end_sec': float(chunk['end_sec']),
                    'duration': float(chunk['duration']),
                    'timestamp': f"{int(chunk['start_sec']//60)}:{int(chunk['start_sec']%60):02d}",
                    
                    # Content
                    'text': chunk['text'][:1000],  # Pinecone metadata limit
                    'text_length': len(chunk['text']),
                    
                    # Source info
                    'url': chunk['url'],
                    'video_title': "Christ Chapel BC Sermon",
                    'channel_title': "Christ Chapel BC",
                    'source': 'christchapelbc',
                    'content_type': 'sermon',
                    
                    # Search helpers
                    'contains_bible': 'bible' in chunk['text'].lower(),
                    'contains_faith': 'faith' in chunk['text'].lower(),
                    'contains_god': 'god' in chunk['text'].lower(),
                    'contains_jesus': 'jesus' in chunk['text'].lower(),
                    'contains_christ': 'christ' in chunk['text'].lower(),
                    'contains_prayer': 'pray' in chunk['text'].lower(),
                }
            }
            vectors.append(vector)
        
        logger.info(f"‚úÖ Vectors prepared with rich metadata")
        return vectors
    
    def upsert_vectors(self, vectors: List[Dict[str, Any]], batch_size: int = 100) -> bool:
        """Upsert vectors with robust error handling."""
        logger.info(f"üöÄ Upserting {len(vectors)} vectors...")
        
        successful_batches = 0
        total_batches = (len(vectors) - 1) // batch_size + 1
        
        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            try:
                self.index.upsert(vectors=batch)
                successful_batches += 1
                logger.info(f"‚úÖ Batch {batch_num}/{total_batches} complete")
                
            except Exception as e:
                logger.error(f"‚ùå Batch {batch_num} failed: {e}")
                continue
        
        logger.info(f"üìä Completed {successful_batches}/{total_batches} batches")
        
        # Get final stats
        try:
            stats = self.index.describe_index_stats()
            if hasattr(stats, 'total_vector_count'):
                logger.info(f"üìà Final index size: {stats.total_vector_count} vectors")
            else:
                logger.info(f"üìà Mock index size: {len(self.index.vectors) if hasattr(self.index, 'vectors') else 'unknown'}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get final stats: {e}")
        
        return successful_batches > 0
    
    def search(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """Perform semantic search."""
        logger.info(f"üîç Searching: '{query[:50]}...'")
        
        try:
            # Create query embedding
            query_embedding = self.create_embeddings([query])[0]
            
            # Search
            results = self.index.query(
                vector=query_embedding.tolist(),
                top_k=top_k,
                include_metadata=True
            )
            
            # Format results
            search_results = []
            for match in results.matches:
                metadata = match.metadata
                
                search_results.append({
                    'score': round(match.score, 3),
                    'video_id': metadata.get('video_id', 'unknown'),
                    'timestamp': metadata.get('timestamp', '0:00'),
                    'start_sec': metadata.get('start_sec', 0),
                    'duration': metadata.get('duration', 0),
                    'url': metadata.get('url', ''),
                    'text': metadata.get('text', ''),
                    'preview': metadata.get('text', '')[:200] + '...',
                    'video_title': metadata.get('video_title', 'Unknown'),
                    'relevance_indicators': {
                        'contains_bible': metadata.get('contains_bible', False),
                        'contains_faith': metadata.get('contains_faith', False), 
                        'contains_god': metadata.get('contains_god', False),
                        'contains_jesus': metadata.get('contains_jesus', False),
                        'contains_christ': metadata.get('contains_christ', False),
                        'contains_prayer': metadata.get('contains_prayer', False),
                    }
                })
            
            logger.info(f"‚úÖ Found {len(search_results)} results")
            
            return {
                'query': query,
                'results': search_results,
                'total_results': len(search_results),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Search failed: {e}")
            return {
                'query': query,
                'results': [],
                'total_results': 0,
                'status': 'error',
                'error': str(e)
            }


def load_all_chunks():
    """Load all processed chunks."""
    files = [
        'processed_Jz1Zb57NUMg.json',
        'processed_6_HgIPUXpVM.json', 
        'processed_iHypAArzphY.json',
        'processed_vM_XX9P66RU.json',
        'processed_OO6l1lkK3yM.json'
    ]
    
    all_chunks = []
    for filename in files:
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                chunks = data.get('chunks', [])
                all_chunks.extend(chunks)
                logger.info(f"‚úÖ {filename}: {len(chunks)} chunks")
    
    return all_chunks


def run_comprehensive_tests(indexer: ProductionIndexer):
    """Run comprehensive search tests."""
    print("\nüß™ COMPREHENSIVE SEARCH TESTS")
    print("=" * 50)
    
    test_queries = [
        "What does the Bible say about faith?",
        "How can I grow in my relationship with God?",
        "What is the gospel message?",
        "How do I pray effectively?",
        "What does it mean to follow Jesus?",
        "How can I find peace in difficult times?",
        "What is God's plan for my life?",
        "How do I read the Bible?",
        "What is salvation?",
        "How can I serve others?"
    ]
    
    results_summary = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nüìã Test {i}: {query}")
        
        result = indexer.search(query, top_k=3)
        
        if result['status'] == 'success' and result['results']:
            print(f"‚úÖ Found {len(result['results'])} results")
            
            for j, match in enumerate(result['results'], 1):
                print(f"   {j}. Score: {match['score']} | {match['timestamp']} | {match['video_id']}")
                print(f"      {match['preview']}")
                print(f"      üîó {match['url']}")
                
                # Show relevance indicators
                indicators = match['relevance_indicators']
                relevant_keys = [k for k, v in indicators.items() if v]
                if relevant_keys:
                    print(f"      üìã Contains: {', '.join(relevant_keys)}")
            
            results_summary.append({
                'query': query,
                'success': True,
                'result_count': len(result['results']),
                'top_score': result['results'][0]['score'] if result['results'] else 0
            })
        else:
            print(f"‚ùå No results found")
            results_summary.append({
                'query': query,
                'success': False,
                'result_count': 0,
                'top_score': 0
            })
    
    # Test summary
    successful_tests = sum(1 for r in results_summary if r['success'])
    avg_score = np.mean([r['top_score'] for r in results_summary if r['success']])
    
    print(f"\nüìä TEST SUMMARY:")
    print(f"   Successful queries: {successful_tests}/{len(test_queries)}")
    print(f"   Success rate: {successful_tests/len(test_queries)*100:.1f}%")
    print(f"   Average top score: {avg_score:.3f}")
    
    return results_summary


def main():
    """Main production indexing pipeline."""
    print("üèõÔ∏è PRODUCTION Christ Chapel BC RAG System")
    print("=" * 70)
    
    try:
        # Initialize
        indexer = ProductionIndexer()
        
        # Load chunks
        chunks = load_all_chunks()
        logger.info(f"üìä Loaded {len(chunks)} total chunks")
        
        # Clear and reindex
        indexer.clear_index()
        
        # Prepare and upsert
        vectors = indexer.prepare_vectors(chunks)
        success = indexer.upsert_vectors(vectors)
        
        if success:
            print(f"\nüéâ INDEXING COMPLETE!")
            print(f"üìä Successfully indexed {len(chunks)} sermon chunks")
            
            # Run comprehensive tests
            test_results = run_comprehensive_tests(indexer)
            
            # Save complete summary
            final_summary = {
                'indexing': {
                    'chunks_processed': len(chunks),
                    'vectors_created': len(vectors),
                    'indexing_successful': success
                },
                'testing': {
                    'tests_run': len(test_results),
                    'successful_tests': sum(1 for r in test_results if r['success']),
                    'success_rate': sum(1 for r in test_results if r['success']) / len(test_results),
                    'average_score': np.mean([r['top_score'] for r in test_results if r['success']])
                },
                'system_status': 'production_ready'
            }
            
            with open('production_summary.json', 'w') as f:
                json.dump(final_summary, f, indent=2)
            
            print(f"\n‚úÖ Production summary: production_summary.json")
            print(f"üöÄ Christ Chapel BC RAG system is PRODUCTION READY!")
            
        else:
            print("‚ùå Indexing failed!")
    
    except Exception as e:
        logger.error(f"‚ùå Production pipeline failed: {e}")
        print(f"‚ùå Error: {e}")


if __name__ == "__main__":
    main()
